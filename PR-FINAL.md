# fix: Complete sanitization and ESM import stabilization

## ç›®çš„
- raw=1/selftest ã® 400/500 ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆ
- å…¨çµŒè·¯ã§ Responses API ã¸ã®ç¦æ­¢ã‚­ãƒ¼é€ä¿¡ã‚’é˜²æ­¢
- ESM import ã‚’ Netlify ã§ç¢ºå®Ÿã«è§£æ±º

## å®Ÿè£…å†…å®¹

### 1. OpenAI ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¼·åˆ¶

```javascript
const BANNED_KEYS = [
  'temperature','top_p','frequency_penalty','presence_penalty',
  'stop','seed','response_format'
];

function sanitizePayload(obj) {
  const clean = JSON.parse(JSON.stringify(obj || {}));
  for (const k of BANNED_KEYS) delete clean[k];
  if (clean.text && 'temperature' in clean.text) delete clean.text.temperature;
  if (clean.response_format) delete clean.response_format;
  return clean;
}

function toResponsesPayload({ input, max_output_tokens }) {
  const capped = Math.min(512, Number(max_output_tokens || 256));
  return sanitizePayload({
    model: process.env.OPENAI_MODEL || 'gpt-5-mini-2025-08-07',
    input,
    max_output_tokens: capped,
    text: { format: { type: 'text' }, verbosity: 'low' },
    reasoning: { effort: 'low' }
  });
}
```

å…¨çµŒè·¯ã§çµ±ä¸€:
```javascript
const finalInput = body?.input ?? (input || toResponsesInputFromMessages(body?.messages || []));
const payload = toResponsesPayload({ input: finalInput, max_output_tokens: body?.max_output_tokens });
// ğŸ”’ ã“ã“ã‚ˆã‚Šå…ˆã¯ payload ã«ç¦æ­¢ã‚­ãƒ¼ãŒç„¡ã„çŠ¶æ…‹
```

### 2. ESM import ã®å®‰å®šåŒ–

```javascript
const path = require('path');
const { pathToFileURL } = require('url');

function fileUrlFromRoot(rel) {
  const base = process.env.LAMBDA_TASK_ROOT || __dirname; // Netlify: /var/task
  return pathToFileURL(path.join(base, rel)).href;
}

let _intentMod, _routerMod, _promptMod;

async function loadIntent() {
  if (!_intentMod) _intentMod = await import(fileUrlFromRoot('src/intent/intent-classifier.mjs'));
  return _intentMod;
}
// åŒæ§˜ã« loadRouter(), loadPrompt()
```

### 3. selftest.js ã®æœ€å°åŒ–

- /chat?raw=1 ã‚’å‘¼ã³å‡ºã—
- é€ä¿¡: `input` ã¨ `max_output_tokens` ã®ã¿ï¼ˆç¦æ­¢ã‚­ãƒ¼ãªã—ï¼‰
- pong å®Œå…¨ä¸€è‡´ã§ ok:trueã€ãã‚Œä»¥å¤–ã¯ 500

### 4. ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½

- `?debug=1` ã§ `payload_keys` è¡¨ç¤º
- `x-sanitized: 1` ãƒ˜ãƒƒãƒ€ãƒ¼
- ç¦æ­¢ã‚­ãƒ¼é™¤å»ã®ç¢ºèªãŒå¯èƒ½

## å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ

`deploy-preview-tests.js` ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ DevTools ã§å®Ÿè¡Œ:

```javascript
// A: raw=1 â†’ "pong"
fetch('/.netlify/functions/chat?raw=1', {
  method:'POST', headers:{'content-type':'application/json'},
  body:JSON.stringify({
    input:[
      {role:'system',content:[{type:'input_text',text:'ã€Œpongã€ã¨1èªã ã‘è¿”ã™'}]},
      {role:'user',content:[{type:'input_text',text:'ping'}]}
    ],
    max_output_tokens:16
  })
}).then(r=>r.text()).then(console.log);

// B: selftest â†’ ok:true
fetch('/.netlify/functions/selftest').then(r=>r.json()).then(console.log);

// C: é€šå¸¸ãƒãƒ£ãƒƒãƒˆï¼ˆæœ¬æ–‡ãŒç©ºã§ãªã„ï¼‰
fetch('/.netlify/functions/chat', {
  method:'POST', headers:{'content-type':'application/json'},
  body:JSON.stringify({messages:[{role:'user',content:'ååˆºã‚’100éƒ¨ä½œã‚ŠãŸã„'}]})
}).then(r=>r.text()).then(console.log);

// D: ãƒ‡ãƒãƒƒã‚°ï¼ˆç¦æ­¢ã‚­ãƒ¼ãŒé™¤å»ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
fetch('/.netlify/functions/chat?raw=1&debug=1', {
  method:'POST', headers:{'content-type':'application/json'},
  body:JSON.stringify({
    input:[
      {role:'system',content:[{type:'input_text',text:'ã€Œpongã€ã¨1èªã ã‘è¿”ã™'}]},
      {role:'user',content:[{type:'input_text',text:'ping'}]}
    ],
    max_output_tokens:16
  })
}).then(r=>r.json()).then(j=>console.log('payload_keys', j.payload_keys));
```

## åˆæ ¼æ¡ä»¶

- âœ… Test A: "pong" ãŒè¿”ã‚‹ (200)
- âœ… Test B: {ok:true, expected:'pong', sample:'pong'} (200)
- âœ… Test C: æ„å‘³ã®ã‚ã‚‹æœ¬æ–‡ï¼ˆæš«å®šã‚¨ãƒ©ãƒ¼ãªã—ï¼‰(200)
- âœ… Test D: payload_keys ã« temperature ç­‰ãŒå«ã¾ã‚Œãªã„

## å½±éŸ¿ç¯„å›²

- æ—¢å­˜ã®ä¼šè©±ãƒ­ã‚¸ãƒƒã‚¯ãƒ»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã«ã¯å½±éŸ¿ãªã—
- ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆå±¤ã®ã¿ã®ä¿®æ­£
- UI äº’æ›æ€§ç¶­æŒï¼ˆchoices[0].message.content å½¢å¼ï¼‰

## Deploy Preview

ä½œæˆå¾Œã«URLã‚’è¿½è¨˜